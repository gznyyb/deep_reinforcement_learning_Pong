{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-T5jiwnV3JrB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Set Up Google Cloud GPU"
      ]
    },
    {
      "metadata": {
        "id": "ueXTMTiUtQcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "8e6ab7f7-73e5-4370-a8d1-884139108668"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-uRENLkutR6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aafd7d31-acc8-453b-f7a9-604bd1763ae9"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: invalid option -- 'o'\n",
            "Try 'mkdir --help' for more information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TyCmQ3BC32Mx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Install Necessary and Import Necessary Packages "
      ]
    },
    {
      "metadata": {
        "id": "D3WL_EGhkgee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#install the necessary packages\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7KzbBemGpB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load in the required packages\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import gym\n",
        "\n",
        "#the wrappers file consists of steps that preprocess the game data ready for \n",
        "#training \n",
        "#the FireResetEnv, MaxAndSkipEnv classes are suggested by \n",
        "#https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "#the other classes are suggested by \n",
        "#https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06/lib\n",
        "import wrappers\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRZZsxHc4JCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training"
      ]
    },
    {
      "metadata": {
        "id": "mfRYgO9B4Neo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Define Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "YmpwfYNwkYF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set up the hyperparameters\n",
        "#hyperparameters are suggested by \n",
        "#https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/02_dqn_pong.py\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19.0\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGkH_70U4TR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Define Neural Network Architectures and Optimization Loss and Method "
      ]
    },
    {
      "metadata": {
        "id": "6-w2QfqiGf9o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "def q_network(x, name):\n",
        "  with tf.variable_scope(name) as scope:\n",
        "    cov1 = tf.layers.conv2d(x, filters=32, kernel_size=8, strides=4,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov1\")\n",
        "    cov2 = tf.layers.conv2d(cov1, filters=64, kernel_size=4, strides=2,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov2\")\n",
        "    cov3 = tf.layers.conv2d(cov2, filters=64, kernel_size=3, strides=1,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov3\")\n",
        "    flat = tf.layers.flatten(cov3)\n",
        "    dense = tf.layers.dense(flat, units=512,activation=tf.nn.relu,name=\"dense1\")\n",
        "    output = tf.layers.dense(dense, units=6, name=\"dense2\")\n",
        "\n",
        "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                       scope=scope.name)\n",
        "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
        "                              for var in trainable_vars}\n",
        "    return output, trainable_vars_by_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JU37bTAQHltw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "82e201f3-37cb-4d14-dfcf-0133c354e847"
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=(None,84,84,4), name=\"train_input\")\n",
        "train_q_values, train_vars = q_network(x, name=\"q_networks/train\")\n",
        "target_q_values, target_vars = q_network(x, name=\"q_networks/target\")\n",
        "\n",
        "copy_ops = [target_var.assign(train_vars[var_name])\n",
        "            for var_name, target_var in target_vars.items()]\n",
        "copy_train_to_target = tf.group(*copy_ops)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-2cb64edd85a6>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-4-2cb64edd85a6>:13: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-2cb64edd85a6>:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WxhyeVV6JR0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "8863aaa7-0fbb-4dec-f7ea-a1679ff97e19"
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"train\"):\n",
        "    a = tf.placeholder(tf.int32, shape=[None])\n",
        "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    q_value = tf.reduce_sum(train_q_values * tf.one_hot(a, env.action_space.n),\n",
        "                            axis=1, keepdims=True)\n",
        "    \n",
        "    #compute the MSE loss based on the q prediction value based on executed action and target value \n",
        "    loss = tf.losses.mean_squared_error(labels=y,predictions=q_value)\n",
        "\n",
        "    #use the Adam Optimizer to minimize the MSE loss\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
        "    train_step = optimizer.minimize(loss)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UDiSS8dE4ldm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Set Up the Experience Buffer and Agent Class"
      ]
    },
    {
      "metadata": {
        "id": "t7OvaFFdOHD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set up a namedtuple object to store one unit of memory \n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self, buffer_size):\n",
        "      #use the deque object to store the experiences. The maxlen argument makes \n",
        "      #sure that old experiences will be eliminated from the replay buffer\n",
        "      #when the buffer reaches a certain size \n",
        "      #the use of deque object is suggested by \n",
        "      #https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
        "      self.buffer = collections.deque(maxlen=buffer_size)\n",
        "      \n",
        "  def __len__(self):\n",
        "      return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      #permute the indices for buffer and take the first number of indices equaling\n",
        "      #to the batch size to sample a random sample from the buffer \n",
        "      indices = np.random.permutation(len(self.buffer))[:batch_size]\n",
        "        \n",
        "      #get the states, actions, rewards, a vector indicating how many games have been\n",
        "      #completed, and next states \n",
        "      states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "      return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "             np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7u6714wONI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, exp_buffer):\n",
        "      self.env = env\n",
        "      self.exp_buffer = exp_buffer\n",
        "      self._reset()\n",
        "\n",
        "  #reset game initial state when a game is done\n",
        "  def _reset(self):\n",
        "      self.state = env.reset()\n",
        "      self.total_reward = 0.0\n",
        "\n",
        "  #get the agent to play a step \n",
        "  def play_step(self, epsilon=0.0):\n",
        "      done_reward = None\n",
        "\n",
        "      #get a random number. If it smaller than epsilon explore the environment with \n",
        "      #a random step \n",
        "      if np.random.random() < epsilon:\n",
        "          action = env.action_space.sample()\n",
        "      else:\n",
        "          state_a = np.array([self.state], copy=False)\n",
        "          state_a.shape = (1,84,84,4)\n",
        "\n",
        "          #evaluate the q value using the q train network\n",
        "          q_vals_v = train_q_values.eval(feed_dict={x: state_a})\n",
        "          action = np.argmax(q_vals_v)\n",
        "\n",
        "      # do step in the environment,execute the step with highest q value\n",
        "      new_state, reward, is_done, _ = self.env.step(action)\n",
        "      self.total_reward += reward\n",
        "\n",
        "      #store this into the experience buffer \n",
        "      exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "      self.exp_buffer.append(exp)\n",
        "      self.state = new_state\n",
        "\n",
        "      #handle end-game situation \n",
        "      if is_done:\n",
        "          done_reward = self.total_reward\n",
        "          self._reset()\n",
        "      return done_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_JI8zCQ4vPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Now We Can Train Our Model"
      ]
    },
    {
      "metadata": {
        "id": "5AchtV7DR2hL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        },
        "outputId": "266a2568-1326-43b6-cf8d-1b6725eb976b"
      },
      "cell_type": "code",
      "source": [
        "#set up the game environment using the wrapper\n",
        "\n",
        "\n",
        "#initialize an experience buffer to store experience \n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "\n",
        "#create an agent \n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "#method used to compute the label\n",
        "method = 'train'\n",
        "\n",
        "#initialize values that will be used for evaluating the model \n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_mean_reward = None\n",
        "\n",
        "epsilon_list = []\n",
        "speed_list = []\n",
        "mean_reward_list = []\n",
        "reward_list = []\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as session:\n",
        "  #if the saved file exists, restore the model. Initialize training\n",
        "  #parameters otherwise\n",
        "  #if os.path.exists('pong_log/test/model.ckpt.index'):\n",
        "  #  saver.restore(session,os.path.join(LOG_DIR, 'test/model.ckpt'))\n",
        "  #  copy_train_to_target.run()\n",
        "  #  EPSILON_START = epsilon\n",
        "  #else:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  #summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), graph)\n",
        "  EPSILON_START = 1.0\n",
        "\n",
        "  #session.run(tf.global_variables_initializer())\n",
        "  #summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), graph)\n",
        "\n",
        "  while True:\n",
        "    frame_idx += 1\n",
        "\n",
        "      #set up a schedule for epsilon according to the number of processed\n",
        "    #frame \n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "      #get the reward of the step \n",
        "    reward = agent.play_step(epsilon)\n",
        "\n",
        "    #record information about speed, epsilon, mean reward, etc \n",
        "    #when each game is finished \n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        mean_reward = np.mean(total_rewards[-100:])\n",
        "        print(\"%d: done %d games, mean reward %.3f, reward %i, eps %.2f, speed %.2f f/s\" % (\n",
        "            frame_idx, len(total_rewards), mean_reward, reward, \n",
        "            epsilon, speed\n",
        "        ))\n",
        "\n",
        "        #record the parameters that show the performance of the model \n",
        "        epsilon_list.append(epsilon)\n",
        "        speed_list.append(speed)\n",
        "        mean_reward_list.append(mean_reward)\n",
        "        reward_list.append(reward)\n",
        "\n",
        "        #record and print out the best mean reward reached and save the model \n",
        "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "            #saver.save(session, os.path.join(LOG_DIR, 'saved_model/model.ckpt'))\n",
        "            if best_mean_reward is not None:\n",
        "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "            best_mean_reward = mean_reward\n",
        "        if mean_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "\n",
        "    #continue to the next loop if the experience buffer is not enough data\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    #copy the weights from the train network to the target network\n",
        "    #periodically \n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        copy_train_to_target.run()\n",
        "\n",
        "    #get a batch from the experience buffer and reshape the states \n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    \n",
        "    states.shape = (BATCH_SIZE,84,84,4)\n",
        "    next_states.shape = (BATCH_SIZE,84,84,4)\n",
        "    \n",
        "    next_q_values = target_q_values.eval(feed_dict={x: next_states})\n",
        "    next_q_value = np.amax(next_q_values, axis=1)\n",
        "    expected_state_action_values = next_q_value * GAMMA + rewards\n",
        "    expected_state_action_values = np.expand_dims(expected_state_action_values,\n",
        "                                                 axis=-1)\n",
        "\n",
        "    #minimize the loss for one step \n",
        "    with tf.device(\"/device:GPU:0\"):\n",
        "      _ = session.run(train_step,feed_dict={x:states,y:expected_state_action_values,a:actions})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1006: done 1 games, mean reward -20.000, reward -20, eps 0.99, speed 150.94 f/s\n",
            "1888: done 2 games, mean reward -20.500, reward -21, eps 0.98, speed 497.59 f/s\n",
            "2669: done 3 games, mean reward -20.667, reward -21, eps 0.97, speed 486.61 f/s\n",
            "3519: done 4 games, mean reward -20.750, reward -21, eps 0.96, speed 486.24 f/s\n",
            "4367: done 5 games, mean reward -20.800, reward -21, eps 0.96, speed 475.37 f/s\n",
            "5157: done 6 games, mean reward -20.833, reward -21, eps 0.95, speed 474.91 f/s\n",
            "5949: done 7 games, mean reward -20.857, reward -21, eps 0.94, speed 466.80 f/s\n",
            "6811: done 8 games, mean reward -20.875, reward -21, eps 0.93, speed 465.17 f/s\n",
            "7663: done 9 games, mean reward -20.889, reward -21, eps 0.92, speed 459.98 f/s\n",
            "8743: done 10 games, mean reward -20.700, reward -19, eps 0.91, speed 465.98 f/s\n",
            "9632: done 11 games, mean reward -20.636, reward -20, eps 0.90, speed 463.94 f/s\n",
            "10484: done 12 games, mean reward -20.667, reward -21, eps 0.90, speed 77.41 f/s\n",
            "11246: done 13 games, mean reward -20.692, reward -21, eps 0.89, speed 47.94 f/s\n",
            "12114: done 14 games, mean reward -20.643, reward -20, eps 0.88, speed 48.54 f/s\n",
            "13104: done 15 games, mean reward -20.600, reward -20, eps 0.87, speed 47.08 f/s\n",
            "14043: done 16 games, mean reward -20.625, reward -21, eps 0.86, speed 48.16 f/s\n",
            "14805: done 17 games, mean reward -20.647, reward -21, eps 0.85, speed 47.14 f/s\n",
            "15766: done 18 games, mean reward -20.667, reward -21, eps 0.84, speed 48.68 f/s\n",
            "16584: done 19 games, mean reward -20.684, reward -21, eps 0.83, speed 48.56 f/s\n",
            "17426: done 20 games, mean reward -20.700, reward -21, eps 0.83, speed 49.21 f/s\n",
            "18461: done 21 games, mean reward -20.619, reward -19, eps 0.82, speed 47.25 f/s\n",
            "19330: done 22 games, mean reward -20.636, reward -21, eps 0.81, speed 47.22 f/s\n",
            "20390: done 23 games, mean reward -20.565, reward -19, eps 0.80, speed 55.31 f/s\n",
            "21323: done 24 games, mean reward -20.583, reward -21, eps 0.79, speed 58.11 f/s\n",
            "22173: done 25 games, mean reward -20.600, reward -21, eps 0.78, speed 57.76 f/s\n",
            "23163: done 26 games, mean reward -20.577, reward -20, eps 0.77, speed 56.98 f/s\n",
            "24353: done 27 games, mean reward -20.481, reward -18, eps 0.76, speed 55.87 f/s\n",
            "25361: done 28 games, mean reward -20.464, reward -20, eps 0.75, speed 56.35 f/s\n",
            "26333: done 29 games, mean reward -20.483, reward -21, eps 0.74, speed 52.90 f/s\n",
            "27259: done 30 games, mean reward -20.500, reward -21, eps 0.73, speed 53.77 f/s\n",
            "28281: done 31 games, mean reward -20.484, reward -20, eps 0.72, speed 53.70 f/s\n",
            "29580: done 32 games, mean reward -20.375, reward -17, eps 0.70, speed 47.66 f/s\n",
            "30432: done 33 games, mean reward -20.394, reward -21, eps 0.70, speed 46.63 f/s\n",
            "31499: done 34 games, mean reward -20.382, reward -20, eps 0.69, speed 46.23 f/s\n",
            "32431: done 35 games, mean reward -20.371, reward -20, eps 0.68, speed 45.88 f/s\n",
            "33329: done 36 games, mean reward -20.389, reward -21, eps 0.67, speed 45.55 f/s\n",
            "34373: done 37 games, mean reward -20.351, reward -19, eps 0.66, speed 45.42 f/s\n",
            "35357: done 38 games, mean reward -20.342, reward -20, eps 0.65, speed 45.02 f/s\n",
            "36674: done 39 games, mean reward -20.231, reward -16, eps 0.63, speed 45.84 f/s\n",
            "38169: done 40 games, mean reward -20.100, reward -15, eps 0.62, speed 46.41 f/s\n",
            "39075: done 41 games, mean reward -20.098, reward -20, eps 0.61, speed 47.73 f/s\n",
            "40392: done 42 games, mean reward -20.048, reward -18, eps 0.60, speed 48.39 f/s\n",
            "41621: done 43 games, mean reward -20.000, reward -18, eps 0.58, speed 46.66 f/s\n",
            "42865: done 44 games, mean reward -20.023, reward -21, eps 0.57, speed 45.89 f/s\n",
            "44354: done 45 games, mean reward -19.911, reward -15, eps 0.56, speed 45.53 f/s\n",
            "Best mean reward updated -20.000 -> -19.911, model saved\n",
            "45511: done 46 games, mean reward -19.891, reward -19, eps 0.54, speed 45.45 f/s\n",
            "Best mean reward updated -19.911 -> -19.891, model saved\n",
            "47123: done 47 games, mean reward -19.745, reward -13, eps 0.53, speed 45.69 f/s\n",
            "Best mean reward updated -19.891 -> -19.745, model saved\n",
            "48396: done 48 games, mean reward -19.750, reward -20, eps 0.52, speed 45.46 f/s\n",
            "49526: done 49 games, mean reward -19.776, reward -21, eps 0.50, speed 45.80 f/s\n",
            "50779: done 50 games, mean reward -19.760, reward -19, eps 0.49, speed 45.31 f/s\n",
            "52206: done 51 games, mean reward -19.725, reward -18, eps 0.48, speed 45.24 f/s\n",
            "Best mean reward updated -19.745 -> -19.725, model saved\n",
            "53439: done 52 games, mean reward -19.712, reward -19, eps 0.47, speed 45.15 f/s\n",
            "Best mean reward updated -19.725 -> -19.712, model saved\n",
            "54393: done 53 games, mean reward -19.736, reward -21, eps 0.46, speed 45.03 f/s\n",
            "55720: done 54 games, mean reward -19.722, reward -19, eps 0.44, speed 45.32 f/s\n",
            "56838: done 55 games, mean reward -19.727, reward -20, eps 0.43, speed 45.09 f/s\n",
            "58316: done 56 games, mean reward -19.661, reward -16, eps 0.42, speed 45.63 f/s\n",
            "Best mean reward updated -19.712 -> -19.661, model saved\n",
            "59530: done 57 games, mean reward -19.649, reward -19, eps 0.40, speed 45.45 f/s\n",
            "Best mean reward updated -19.661 -> -19.649, model saved\n",
            "60680: done 58 games, mean reward -19.672, reward -21, eps 0.39, speed 45.22 f/s\n",
            "61740: done 59 games, mean reward -19.678, reward -20, eps 0.38, speed 44.65 f/s\n",
            "62860: done 60 games, mean reward -19.700, reward -21, eps 0.37, speed 45.28 f/s\n",
            "64333: done 61 games, mean reward -19.656, reward -17, eps 0.36, speed 45.73 f/s\n",
            "65951: done 62 games, mean reward -19.597, reward -16, eps 0.34, speed 45.94 f/s\n",
            "Best mean reward updated -19.649 -> -19.597, model saved\n",
            "67409: done 63 games, mean reward -19.587, reward -19, eps 0.33, speed 47.26 f/s\n",
            "Best mean reward updated -19.597 -> -19.587, model saved\n",
            "68865: done 64 games, mean reward -19.562, reward -18, eps 0.31, speed 47.76 f/s\n",
            "Best mean reward updated -19.587 -> -19.562, model saved\n",
            "70385: done 65 games, mean reward -19.523, reward -17, eps 0.30, speed 50.18 f/s\n",
            "Best mean reward updated -19.562 -> -19.523, model saved\n",
            "71856: done 66 games, mean reward -19.515, reward -19, eps 0.28, speed 48.99 f/s\n",
            "Best mean reward updated -19.523 -> -19.515, model saved\n",
            "73690: done 67 games, mean reward -19.448, reward -15, eps 0.26, speed 45.43 f/s\n",
            "Best mean reward updated -19.515 -> -19.448, model saved\n",
            "75473: done 68 games, mean reward -19.456, reward -20, eps 0.25, speed 44.63 f/s\n",
            "77464: done 69 games, mean reward -19.391, reward -15, eps 0.23, speed 44.63 f/s\n",
            "Best mean reward updated -19.448 -> -19.391, model saved\n",
            "79810: done 70 games, mean reward -19.329, reward -15, eps 0.20, speed 45.05 f/s\n",
            "Best mean reward updated -19.391 -> -19.329, model saved\n",
            "81881: done 71 games, mean reward -19.211, reward -11, eps 0.18, speed 49.91 f/s\n",
            "Best mean reward updated -19.329 -> -19.211, model saved\n",
            "83327: done 72 games, mean reward -19.181, reward -17, eps 0.17, speed 51.65 f/s\n",
            "Best mean reward updated -19.211 -> -19.181, model saved\n",
            "84695: done 73 games, mean reward -19.178, reward -19, eps 0.15, speed 48.47 f/s\n",
            "Best mean reward updated -19.181 -> -19.178, model saved\n",
            "86282: done 74 games, mean reward -19.149, reward -17, eps 0.14, speed 45.79 f/s\n",
            "Best mean reward updated -19.178 -> -19.149, model saved\n",
            "88178: done 75 games, mean reward -19.147, reward -19, eps 0.12, speed 43.96 f/s\n",
            "Best mean reward updated -19.149 -> -19.147, model saved\n",
            "89664: done 76 games, mean reward -19.132, reward -18, eps 0.10, speed 44.96 f/s\n",
            "Best mean reward updated -19.147 -> -19.132, model saved\n",
            "91616: done 77 games, mean reward -19.091, reward -16, eps 0.08, speed 44.70 f/s\n",
            "Best mean reward updated -19.132 -> -19.091, model saved\n",
            "93769: done 78 games, mean reward -19.000, reward -12, eps 0.06, speed 44.23 f/s\n",
            "Best mean reward updated -19.091 -> -19.000, model saved\n",
            "95640: done 79 games, mean reward -18.987, reward -18, eps 0.04, speed 46.15 f/s\n",
            "Best mean reward updated -19.000 -> -18.987, model saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}