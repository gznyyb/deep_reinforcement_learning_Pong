{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong_rl_notes1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Olq_QJ27Vgtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Final Project: Deep Reinforcement Learning With Atari Pong Game"
      ]
    },
    {
      "metadata": {
        "id": "nVYLKTK-X4Bp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Preparation"
      ]
    },
    {
      "metadata": {
        "id": "qddwspOGX8bO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Set Google Colab GPU"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OZe35l1aFFuA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V5fQftWRF3E5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PHfmELaa-nXU"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load packages & files"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n0T9xdBFDIv-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#install the necessary packages\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UAv43azEYuLd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load in the required packages\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import gym\n",
        "\n",
        "#the wrappers file consists of steps that preprocess the game data ready for \n",
        "#training \n",
        "#the FireResetEnv, MaxAndSkipEnv classes are suggested by \n",
        "#https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "#the other classes are suggested by \n",
        "#https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06/lib\n",
        "import wrappers\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F0hXFaow3YRR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create a folder for storing the training data \n",
        "LOG_DIR = './pong_log'\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5gJOZrn7oUDB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set up the hyperparameters\n",
        "#hyperparameters are suggested by \n",
        "#https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/02_dqn_pong.py\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 16.0\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "icj_BkJr-uGH"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Training\n",
        "\n",
        "### 2.1 Training A Reinforcement Learning Agent Playing Pong"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5rTsvgkz8_NL"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1 The Training Network and The Target Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o23rlw4pZob_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#convolutional network\n",
        "#note that the architecture is suggested by https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    \n",
        "  #set up placeholders to feed in the training data\n",
        "  x = tf.placeholder(tf.float32,shape=(None,84,84,4),name=\"train_input\")\n",
        "  y = tf.placeholder(tf.float32,shape=(None),name=\"labels\")\n",
        "  a = tf.placeholder(tf.int32,shape=(None,2),name=\"labels\")\n",
        "  x_target = tf.placeholder(tf.float32,shape=(None,84,84,4),name=\"target_input\")\n",
        "    \n",
        "  #set up a training network for training an approximation to the Q function\n",
        "  #the network consists of three convolutional layers and two dense layers \n",
        "  with tf.variable_scope(\"train\") as scope:\n",
        "    cov1 = tf.layers.conv2d(x, filters=32, kernel_size=8, strides=4,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov1\")\n",
        "    cov2 = tf.layers.conv2d(cov1, filters=64, kernel_size=4, strides=2,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov2\")\n",
        "    cov3 = tf.layers.conv2d(cov2, filters=64, kernel_size=3, strides=1,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov3\")\n",
        "    flat = tf.layers.flatten(cov3)\n",
        "    dense = tf.layers.dense(flat, units=512,activation=tf.nn.relu,name=\"dense1\")\n",
        "    q_values = tf.layers.dense(dense, units=6, name=\"dense2\")\n",
        "    #output the q value prediction for the executed action\n",
        "    q_values_flat = tf.gather_nd(q_values,a)\n",
        "\n",
        "  #set up a target network with the same type of architecture but is used for \n",
        "  #updating the Q value training target using the Bellman equation \n",
        "  with tf.variable_scope(\"target\") as scope:\n",
        "    cov1_target = tf.layers.conv2d(x_target, filters=32, kernel_size=8, strides=4,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov1\")\n",
        "    cov2_target = tf.layers.conv2d(cov1_target, filters=64, kernel_size=4, strides=2,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov2\")\n",
        "    cov3_target = tf.layers.conv2d(cov2_target, filters=64, kernel_size=3, strides=1,\n",
        "                         padding='same', activation=tf.nn.relu,name=\"cov3\")\n",
        "    flat_target = tf.layers.flatten(cov3_target)\n",
        "    dense_target = tf.layers.dense(flat_target, units=512,activation=tf.nn.relu,name=\"dense1\")\n",
        "    q_values_target = tf.layers.dense(dense_target, units=6, name=\"dense2\")  \n",
        "    \n",
        "  #compute the MSE loss based on the q prediction value based on executed action and target value \n",
        "  loss = tf.losses.mean_squared_error(labels=y,predictions=q_values_flat)\n",
        "\n",
        "  #use the Adam Optimizer to minimize the MSE loss\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
        "  train_step = optimizer.minimize(loss)\n",
        "\n",
        "  #matrices used for evaluating the prediction and target q values \n",
        "  output_target = graph.get_tensor_by_name('target/dense2/BiasAdd:0')\n",
        "  output_train = graph.get_tensor_by_name('train/dense2/BiasAdd:0')\n",
        "\n",
        "\n",
        "  \n",
        "#summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'graph'), graph)\n",
        "#summary_writer.close()\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n_oVgIrW9Dv2"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.2 Set Up Operation to Copy Weights From The Training Network to The Target Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nuJ6u0ooyi0f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the tensorflow code responsible for copying weights from one network to another is\n",
        "#suggested by https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
        "with graph.as_default():\n",
        "  train_vars = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='train')\n",
        "  train_var_dict = {var.name[len('train'):]:var for var in train_vars}\n",
        "\n",
        "  target_vars = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='target')\n",
        "  target_var_dict = {var.name[len('target'):]:var for var in target_vars}\n",
        "\n",
        "  copy_ops = [targ_var.assign(train_var_dict[var_name]) \n",
        "              for var_name,targ_var in target_var_dict.items()]\n",
        "  copy_train_to_target = tf.group(*copy_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9V7h5b-09HS4"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.3 Set Up The Experience Buffer For Storing Agent's Experience"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l34IQy1mZvI0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set up a namedtuple object to store one unit of memory \n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self, buffer_size):\n",
        "      #use the deque object to store the experiences. The maxlen argument makes \n",
        "      #sure that old experiences will be eliminated from the replay buffer\n",
        "      #when the buffer reaches a certain size \n",
        "      #the use of deque object is suggested by \n",
        "      #https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
        "      self.buffer = collections.deque(maxlen=buffer_size)\n",
        "      \n",
        "  def __len__(self):\n",
        "      return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      #permute the indices for buffer and take the first number of indices equaling\n",
        "      #to the batch size to sample a random sample from the buffer \n",
        "      indices = np.random.permutation(len(self.buffer))[:batch_size]\n",
        "        \n",
        "      #get the states, actions, rewards, a vector indicating how many games have been\n",
        "      #completed, and next states \n",
        "      states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "      return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "             np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UwKRUCnE9L6q"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.4 Set Up The Agent to Execute A Step According to Action With Highest Q Value Or Explore the Environment Randomly"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AH3K_fwqolYR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, exp_buffer):\n",
        "      self.env = env\n",
        "      self.exp_buffer = exp_buffer\n",
        "      self._reset()\n",
        "\n",
        "  #reset game initial state when a game is done\n",
        "  def _reset(self):\n",
        "      self.state = env.reset()\n",
        "      self.total_reward = 0.0\n",
        "\n",
        "  #get the agent to play a step \n",
        "  def play_step(self, session, epsilon=0.0):\n",
        "      done_reward = None\n",
        "\n",
        "      #get a random number. If it smaller than epsilon explore the environment with \n",
        "      #a random step \n",
        "      if np.random.random() < epsilon:\n",
        "          action = env.action_space.sample()\n",
        "      else:\n",
        "          state_a = np.array([self.state], copy=False)\n",
        "          state_a.shape = (1,84,84,4)\n",
        "\n",
        "          #evaluate the q value using the q train network\n",
        "          q_vals_v = session.run(output_train,feed_dict={'train_input:0':state_a})\n",
        "          action = np.argmax(q_vals_v)\n",
        "\n",
        "      # do step in the environment,execute the step with highest q value\n",
        "      new_state, reward, is_done, _ = self.env.step(action)\n",
        "      self.total_reward += reward\n",
        "      new_state = new_state\n",
        "\n",
        "      #store this into the experience buffer \n",
        "      exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "      self.exp_buffer.append(exp)\n",
        "      self.state = new_state\n",
        "\n",
        "      #handle end-game situation \n",
        "      if is_done:\n",
        "          done_reward = self.total_reward\n",
        "          self._reset()\n",
        "      return done_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vzoyHR4_9Pgo"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.5 Get The Action Executed and The Target Q Value"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xs-cZ5zcooql",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_action_label(session,batch,method='simple'):\n",
        "    #get the states, actions, rewards earned, and next states from the experience\n",
        "    #buffer \n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    \n",
        "    #adjust the shape of the states tensor so that it conforms to the tensorflow\n",
        "    #standard since the preprocessing in wrappers file works for pytorch not tensorflow \n",
        "    next_states.shape = (BATCH_SIZE,84,84,4)\n",
        "\n",
        "    actions_index = np.zeros(2)\n",
        "    for n_row,_ in enumerate(actions):\n",
        "        actions_index = np.vstack((actions_index,np.array((n_row,actions[n_row]))))\n",
        "    actions_index = np.delete(actions_index,0,axis=0)\n",
        "    actions_index = np.array(actions_index,dtype='int64')\n",
        "    \n",
        "    #use the train network to evaluate q value for next state\n",
        "    if method=='train':\n",
        "      #evaluate next state q value using the train network\n",
        "      next_state_values = session.run(output_train,feed_dict={'train_input:0':next_states})\n",
        "    \n",
        "      #evaluate the best q values\n",
        "      next_state_values = np.amax(next_state_values,axis=1)\n",
        "    \n",
        "    #use the target network to evaluate q value for next state\n",
        "    #the use of target network is suggested by https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
        "    elif method=='simple':\n",
        "      #evaluate next state q value using the train network\n",
        "      next_state_values = session.run(output_target,feed_dict={'target_input:0':next_states})\n",
        "    \n",
        "      #evaluate the best q values \n",
        "      next_state_values = np.amax(next_state_values,axis=1)\n",
        "      \n",
        "    #use the double q network architecture to evaluate next state \n",
        "    #the double q learning method is suggested by https://arxiv.org/pdf/1509.06461.pdf\n",
        "    #to reduce the impact of overestimation of the q action values by simple deep q learning \n",
        "    elif method=='double':\n",
        "      next_state_values_from_train_net = session.run(output_train,\n",
        "                                                   feed_dict={'train_input:0':next_states})\n",
        "      #get the action with the best value \n",
        "      best_action_from_train_net = np.argmax(next_state_values_from_train_net,axis=1)\n",
        "\n",
        "      #evaluate the q value of this \"best action\" using the target network\n",
        "      next_state_values_from_target_net = session.run(output_target,\n",
        "                                                      feed_dict={'target_input:0':next_states})\n",
        "      next_state_values = next_state_values_from_target_net[np.arange(BATCH_SIZE),\n",
        "                                                            best_action_from_train_net]\n",
        "\n",
        "    #handle end-game situation where q value of next state is 0 \n",
        "    next_state_values[dones] = 0.0\n",
        "\n",
        "    #calculate the expected q value, aka the labels for training, using the \n",
        "    #Bellman equation \n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards\n",
        "    return actions_index,expected_state_action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_GqAJAya9RoX"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Train The Agent "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tKKnve8EosMy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #set up the game environment using the wrapper\n",
        "  env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "  #initialize an experience buffer to store experience \n",
        "  buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "\n",
        "  #create an agent \n",
        "  agent = Agent(env, buffer)\n",
        "  epsilon = EPSILON_START\n",
        "  \n",
        "  #method used to compute the label\n",
        "  method = 'train'\n",
        "\n",
        "  #initialize values that will be used for evaluating the model \n",
        "  total_rewards = []\n",
        "  frame_idx = 0\n",
        "  ts_frame = 0\n",
        "  ts = time.time()\n",
        "  best_mean_reward = None\n",
        "\n",
        "  epsilon_list = []\n",
        "  speed_list = []\n",
        "  mean_reward_list = []\n",
        "  reward_list = []\n",
        "\n",
        "  with graph.as_default():\n",
        "    \n",
        "    #set up a saver to save the training parameters \n",
        "    saver = tf.train.Saver()\n",
        "  \n",
        "    with tf.Session() as session:\n",
        "      #if the saved file exists, restore the model. Initialize training\n",
        "      #parameters otherwise\n",
        "      if os.path.exists('pong_log/test/model.ckpt.index'):\n",
        "        saver.restore(session,os.path.join(LOG_DIR, 'test/model.ckpt'))\n",
        "        copy_train_to_target.run()\n",
        "        EPSILON_START = epsilon\n",
        "      else:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), graph)\n",
        "        EPSILON_START = 1.0\n",
        "\n",
        "      #session.run(tf.global_variables_initializer())\n",
        "      #summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), graph)\n",
        "\n",
        "      while True:\n",
        "        frame_idx += 1\n",
        "\n",
        "          #set up a schedule for epsilon according to the number of processed\n",
        "        #frame \n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "          #get the reward of the step \n",
        "        reward = agent.play_step(session, epsilon)\n",
        "\n",
        "        #record information about speed, epsilon, mean reward, etc \n",
        "        #when each game is finished \n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, mean reward %.3f, reward %i, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, reward, \n",
        "                epsilon, speed\n",
        "            ))\n",
        "\n",
        "            #record the parameters that show the performance of the model \n",
        "            epsilon_list.append(epsilon)\n",
        "            speed_list.append(speed)\n",
        "            mean_reward_list.append(mean_reward)\n",
        "            reward_list.append(reward)\n",
        "\n",
        "            #record and print out the best mean reward reached and save the model \n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                saver.save(session, os.path.join(LOG_DIR, 'saved_model/model.ckpt'))\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        #continue to the next loop if the experience buffer is not enough data\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        #copy the weights from the train network to the target network\n",
        "        #periodically \n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            copy_train_to_target.run()\n",
        "\n",
        "        #get a batch from the experience buffer and reshape the states \n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        batch[0].shape = (BATCH_SIZE,84,84,4)\n",
        "\n",
        "        #get the actions and labels for each batch depending on the method used \n",
        "        act_index,expected = get_action_label(session, batch,method=method)\n",
        "\n",
        "        #minimize the loss for one step \n",
        "        with tf.device(\"/device:GPU:0\"):\n",
        "          _ = session.run(train_step,feed_dict={x:batch[0],y:expected,a:act_index})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uLyswBPy2sc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##3. References"
      ]
    },
    {
      "metadata": {
        "id": "O52bAjvPy7Hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.   https://gym.openai.com/envs/Pong-v0/\n",
        "2.   https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "3. https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06/lib\n",
        "4. https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/02_dqn_pong.py\n",
        "5. https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
        "6. https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
        "7. https://arxiv.org/pdf/1509.06461.pdf\n"
      ]
    }
  ]
}